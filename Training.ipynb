{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dog vs Cat Convolution Neural Network Classifier\n",
        "Problem statement :\n",
        "In this Section we are implementing Convolution Neural Network(CNN) Classifier for Classifying dog and cat images. The Total number of images available for training is 25,000 and final testing is done on seperate 10,000 images.\n",
        "\n",
        "Note:This problem statement and dataset is taken from this Kaggle competition.\n",
        "Dependencies\n",
        "Jupyter notebook\n",
        "Tensorflow 1.10\n",
        "Python 3.6\n",
        "Matplotlib\n",
        "Seaborn\n",
        "Scikit-Learn\n",
        "Pandas\n",
        "Numpy\n",
        "Install dependencies using conda\n",
        "\n",
        "Test Train Split\n",
        "Image training set contain 12500 images for each category. I split those into 80% train and 20% means test Split each class images into 10,000 for train and 2,500 for test.\n",
        "\n",
        "data set can be downloaded from below link\n",
        "#https://www.kaggle.com/c/dogs-vs-cats"
      ],
      "metadata": {
        "id": "e-tuFyry3XdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "05vgOmuC3mLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ONsY4Q763r6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network Parameter:\n",
        "* Rectifier Linear Unit \n",
        "* Adam optimizer\n",
        "* Sigmoid on Final output\n",
        "* Binary CrossEntropy loss"
      ],
      "metadata": {
        "id": "G0gWPWl03u8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the model\n",
        "classifier = Sequential()\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2),strides=2)) #if stride not given it equal to pool filter size\n",
        "classifier.add(Conv2D(32,(3,3),activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(units=128,activation='relu'))\n",
        "classifier.add(Dense(units=1,activation='sigmoid'))\n",
        "adam = tensorflow.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "classifier.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
      ],
      "metadata": {
        "id": "cb6jaUBO3x0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "Using some Data Augmentation techniques for more data and Better results.\n",
        "* Shearing of images\n",
        "* Random zoom\n",
        "* Horizontal flips"
      ],
      "metadata": {
        "id": "ByyTotNJ34cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read the dataset saved locally, create one test1 folder and keep some 1000 images for testing \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.1,\n",
        "                                   zoom_range=0.1,\n",
        "                                   horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#Training Set\n",
        "train_set = train_datagen.flow_from_directory('train',\n",
        "                                             target_size=(64,64),\n",
        "                                             batch_size=128,\n",
        "                                             class_mode='binary')\n",
        "#Validation Set\n",
        "test_set = test_datagen.flow_from_directory('test',\n",
        "                                           target_size=(64,64),\n",
        "                                           batch_size = 128,\n",
        "                                           class_mode='binary',\n",
        "                                           shuffle=False)\n",
        "#Test Set /no output available\n",
        "test_set1 = test_datagen.flow_from_directory('test1',\n",
        "                                            target_size=(64,64),\n",
        "                                            batch_size=128,\n",
        "                                            shuffle=False)"
      ],
      "metadata": {
        "id": "t1abV_SA38pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "model_file_path = 'dog_cat_model_Final_fulldatset.h5'\n",
        "model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    model_file_path,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    mode=\"min\",\n",
        ")"
      ],
      "metadata": {
        "id": "YE02Yro64C5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the model\n",
        "history = classifier.fit_generator(train_set,\n",
        "                        steps_per_epoch=len(train_set), \n",
        "                        epochs = 50,\n",
        "                        validation_data = test_set,\n",
        "                        validation_steps = len(test_set), \n",
        "                        verbose=1,\n",
        "                        callbacks=[model_checkpoint]\n",
        "                        );"
      ],
      "metadata": {
        "id": "P-Mjhy724F2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}